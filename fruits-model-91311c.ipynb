{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom keras.utils import img_to_array, load_img\nimport numpy as np\nimport cv2 as cv\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\n# # Read in example image and get shape\n# img = load_img(trainPath + \"/Quince/0_100.jpg\")\n# plt.imshow(img)\n# plt.show()\n\n# imgA = img_to_array(img)\n# print(imgA.shape)        \n# Build model\nmodel = Sequential()\nmodel.add(Conv2D(filters=128, kernel_size=3, activation=\"relu\", input_shape=(100,100,3)))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\"))\nmodel.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(5000, activation=\"relu\"))\nmodel.add(Dense(1000, activation=\"relu\"))\nmodel.add(Dense(131, activation=\"softmax\"))\n\nprint(model.summary())\n\n\n# Compile model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n\nstepsPerEpoch = np.ceil(train_generator.samples / batchSize)\nvalidationSteps = np.ceil(test_generator.samples / batchSize)\n\n# Early stopping\nstop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\nhistory = model.fit(train_generator, steps_per_epoch=stepsPerEpoch, epochs=5, validation_data=test_generator, validation_steps=validationSteps, callbacks=[stop_early])\n\nmodel.save(\"/kaggle/working/fruits360.h5\")     # Add file path to save the model to","metadata":{"_uuid":"8bc14dc0-1c02-4a72-96c2-bf27321b7876","_cell_guid":"6a03741b-6dd5-435e-8d8d-ffb6989d96da","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T22:37:24.268025Z","iopub.execute_input":"2023-07-02T22:37:24.268416Z","iopub.status.idle":"2023-07-02T23:02:47.068551Z","shell.execute_reply.started":"2023-07-02T22:37:24.268381Z","shell.execute_reply":"2023-07-02T23:02:47.067527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing model","metadata":{}},{"cell_type":"code","source":"!pip uninstall opencv-python-headless -y \n!pip install opencv-python --upgrade\nimport tensorflow as tf\nimport os\nfrom keras.utils import img_to_array, load_img\nimport numpy as np\nimport cv2 as cv\n\n# Load model\nmodel = tf.keras.models.load_model(\"/kaggle/working/fruits360.h5\")       # Put in model path here\nprint(model.summary())\n\n# Load categories\nsource_folder = \"/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test\"\ncategories = os.listdir(source_folder)\n# categories = categories.sort()\nprint(categories)\nprint(len(categories))      # Should be 131\n\n# Load and prepare image\ndef prepareImage(path):\n    img = load_img(path, target_size=(100,100))\n    imgArray = img_to_array(img)\n    # print(imgArray.shape)\n    imgArray = np.expand_dims(imgArray, axis=0)\n    imgArray = imgArray / 255.\n    return imgArray\n\ntestImgPath = \"/kaggle/input/eggplant/eggplant.jpg\"\nimageForModel = prepareImage(testImgPath)\n\nresultArray = model.predict(imageForModel, verbose=1)\nanswers = np.argmax(resultArray, axis=1)\nprint(categories[answers[0]])\n\n# Show image with text below\nimg = cv.imread(testImgPath)\ncv.putText(img, categories[answers[0]],(0,50), cv.FONT_HERSHEY_COMPLEX, 1, (209, 19, 77), 2)\ncv.imshow('img', img)\ncv.waitKey(0)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T21:09:07.305597Z","iopub.status.idle":"2023-07-13T21:09:07.305993Z","shell.execute_reply.started":"2023-07-13T21:09:07.305785Z","shell.execute_reply":"2023-07-13T21:09:07.305803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow-gpu==2.10\nfrom tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:25:29.944169Z","iopub.execute_input":"2023-07-14T20:25:29.944556Z","iopub.status.idle":"2023-07-14T21:05:35.648412Z","shell.execute_reply.started":"2023-07-14T20:25:29.944522Z","shell.execute_reply":"2023-07-14T21:05:35.645350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning 2","metadata":{}},{"cell_type":"code","source":"import keras_tuner as kt\nimport tensorflow as tf\nimport os\nfrom keras.utils import img_to_array, load_img\nimport numpy as np\nimport cv2 as cv\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\n\n#class MyHyperModel(kt.HyperModel):\ndef build(hp):\n    model = Sequential()\n    model.add(Conv2D(filters=128, kernel_size=3, activation=hp.Choice('activation', ['relu', 'PReLU', 'elu']), input_shape=(100,100,3)))\n    model.add(MaxPooling2D())\n    model.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\"))\n    model.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n    model.add(MaxPooling2D())\n    model.add(Dropout(rate=hp.Float('rate',min_value=0.3,max_value=0.8,step=0.1)))\n    model.add(Flatten())\n    model.add(Dense(units=hp.Int('units',3000,8000,500), activation=hp.Choice('activation_2', ['relu', 'PReLU', 'elu'])))\n    model.add(Dense(units=hp.Int('units_2',500,1500,250), activation=\"relu\"))\n    model.add(Dense(131, activation=\"softmax\"))\n    #model.add(keras.layers.Dense(\n     #   hp.Choice('units', [8, 16, 32]),\n      #  activation='relu'))\n   # model.add(keras.layers.Dense(1, activation='relu'))\n    model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n    \n    print(model.summary())\n    return model\n\n#def fit(hp, model):\n    # Early stopping\n#stop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\n#history = model.fit(train_generator, steps_per_epoch=stepsPerEpoch, epochs=5, validation_data=test_generator, validation_steps=validationSteps, callbacks=[stop_early])\n\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n#x = MyHyperMode(kt.HyperModel)\n#build(kt.HyperParameters())\n#tuner = kt.Hyperband(hypermodel = build, objective=\"val_accuracy\", max_epochs=10, hyperband_iterations=10,seed=1,max_retries_per_trial=0, max_consecutive_failed_trials=3)\ntuner = kt.RandomSearch(hypermodel = build, objective=\"val_accuracy\", max_trials=10,executions_per_trial=1,overwrite=False)\n#tuner.search_space_summary()\ntuner.search(train_generator,epochs=2,validation_data=test_generator)\ntuner.results_summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:57:55.346565Z","iopub.execute_input":"2023-07-16T17:57:55.346981Z","iopub.status.idle":"2023-07-16T19:25:22.615484Z","shell.execute_reply.started":"2023-07-16T17:57:55.346944Z","shell.execute_reply":"2023-07-16T19:25:22.613901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VGG 16","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom keras.models import Model\nfrom keras.utils import img_to_array, load_img\nfrom keras.optimizers import Adam\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Dense, Dropout, Flatten\nfrom pathlib import Path\n#from livelossplot.inputs.keras import PlotLossesCallback\nimport numpy as np\n\n#import cv2 as cv\n#from keras.models import Sequential\n#from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n#import matplotlib.pyplot as plt\n\n\nBATCH_SIZE = 64\n\n'''train_datagen = ImageDataGenerator(rotation_range=90, \n                                     brightness_range=[0.1, 0.7],\n                                     width_shift_range=0.5, \n                                     height_shift_range=0.5,\n                                     horizontal_flip=True, \n                                     vertical_flip=True,\n                                     validation_split=0.15,\n                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing\n'''\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n\ndef create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n    \"\"\"\n    Compiles a model integrated with VGG16 pretrained layers\n    \n    input_shape: tuple - the shape of input images (width, height, channels)\n    n_classes: int - number of classes for the output layer\n    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n    fine_tune: int - The number of pre-trained layers to unfreeze.\n                If set to 0, all pretrained layers will freeze during training\n    \"\"\"\n    \n    # Pretrained convolutional layers are loaded using the Imagenet weights.\n    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n    conv_base = VGG16(include_top=False,\n                     weights='imagenet', \n                     input_shape=input_shape)\n    \n    # Defines how many layers to freeze during training.\n    # Layers in the convolutional base are switched from trainable to non-trainable\n    # depending on the size of the fine-tuning parameter.\n    if fine_tune > 0:\n        for layer in conv_base.layers[:-fine_tune]:\n            layer.trainable = False\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n            \n    # Create a new 'top' of the model (i.e. fully-connected layers).\n    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n    top_model = conv_base.output\n    top_model = Flatten(name=\"flatten\")(top_model)\n    top_model = Dense(5000, activation='relu')(top_model)\n    top_model = Dense(1000, activation='relu')(top_model)\n    top_model = Dropout(0.2)(top_model)\n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    # Group the convolutional base and new fully-connected layers into a Model object.\n    model = Model(inputs=conv_base.input, outputs=output_layer)\n\n    # Compiles the model for training.\n    model.compile(optimizer=optimizer, \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    print(model.summary())\n    return model\n\ninput_shape = (100, 100, 3)\noptim_1 = Adam(learning_rate=0.001)\nn_classes=131\n\nn_steps = train_generator.samples // batchSize\nn_val_steps = test_generator.samples // batchSize\nn_epochs = 50\n\n# First we'll train the model without Fine-tuning\nvgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n\n\n# Early stopping\nstop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\nhistory = vgg_model.fit(train_generator, steps_per_epoch=n_steps, epochs=5, validation_data=test_generator, validation_steps=n_val_steps, callbacks=[stop_early])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:34:44.008337Z","iopub.execute_input":"2023-07-18T21:34:44.008700Z","iopub.status.idle":"2023-07-18T22:10:07.174176Z","shell.execute_reply.started":"2023-07-18T21:34:44.008671Z","shell.execute_reply":"2023-07-18T22:10:07.173137Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Found 67692 images belonging to 131 classes.\nFound 22688 images belonging to 131 classes.\nModel: \"model_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_17 (InputLayer)       [(None, 100, 100, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 4608)              0         \n                                                                 \n dense_30 (Dense)            (None, 5000)              23045000  \n                                                                 \n dense_31 (Dense)            (None, 1000)              5001000   \n                                                                 \n dropout_10 (Dropout)        (None, 1000)              0         \n                                                                 \n dense_32 (Dense)            (None, 131)               131131    \n                                                                 \n=================================================================\nTotal params: 42,891,819\nTrainable params: 28,177,131\nNon-trainable params: 14,714,688\n_________________________________________________________________\nNone\nEpoch 1/5\n1057/1057 [==============================] - 880s 832ms/step - loss: 1.1622 - accuracy: 0.6875 - val_loss: 0.7355 - val_accuracy: 0.8033\nEpoch 2/5\n1057/1057 [==============================] - 315s 297ms/step - loss: 0.3034 - accuracy: 0.9003 - val_loss: 0.4199 - val_accuracy: 0.8874\nEpoch 3/5\n1057/1057 [==============================] - 302s 286ms/step - loss: 0.2228 - accuracy: 0.9270 - val_loss: 0.3915 - val_accuracy: 0.8974\nEpoch 4/5\n1057/1057 [==============================] - 301s 285ms/step - loss: 0.1934 - accuracy: 0.9355 - val_loss: 0.3411 - val_accuracy: 0.9099\nEpoch 5/5\n1057/1057 [==============================] - 300s 284ms/step - loss: 0.1682 - accuracy: 0.9441 - val_loss: 0.4242 - val_accuracy: 0.8900\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}