{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom keras.utils import img_to_array, load_img\nimport numpy as np\nimport cv2 as cv\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\n# # Read in example image and get shape\n# img = load_img(trainPath + \"/Quince/0_100.jpg\")\n# plt.imshow(img)\n# plt.show()\n\n# imgA = img_to_array(img)\n# print(imgA.shape)        \n# Build model\nmodel = Sequential()\nmodel.add(Conv2D(filters=128, kernel_size=3, activation=\"relu\", input_shape=(100,100,3)))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\"))\nmodel.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(5000, activation=\"relu\"))\nmodel.add(Dense(1000, activation=\"relu\"))\nmodel.add(Dense(131, activation=\"softmax\"))\n\nprint(model.summary())\n\n\n# Compile model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n\nstepsPerEpoch = np.ceil(train_generator.samples / batchSize)\nvalidationSteps = np.ceil(test_generator.samples / batchSize)\n\n# Early stopping\nstop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\nhistory = model.fit(train_generator, steps_per_epoch=stepsPerEpoch, epochs=5, validation_data=test_generator, validation_steps=validationSteps, callbacks=[stop_early])\n\nmodel.save(\"/kaggle/working/fruits360.h5\")     # Add file path to save the model to","metadata":{"_uuid":"8bc14dc0-1c02-4a72-96c2-bf27321b7876","_cell_guid":"6a03741b-6dd5-435e-8d8d-ffb6989d96da","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T22:37:24.268025Z","iopub.execute_input":"2023-07-02T22:37:24.268416Z","iopub.status.idle":"2023-07-02T23:02:47.068551Z","shell.execute_reply.started":"2023-07-02T22:37:24.268381Z","shell.execute_reply":"2023-07-02T23:02:47.067527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing model","metadata":{}},{"cell_type":"code","source":"!pip uninstall opencv-python-headless -y \n!pip install opencv-python --upgrade\nimport tensorflow as tf\nimport os\nfrom keras.utils import img_to_array, load_img\nimport numpy as np\nimport cv2 as cv\n\n# Load model\nmodel = tf.keras.models.load_model(\"/kaggle/working/fruits360.h5\")       # Put in model path here\nprint(model.summary())\n\n# Load categories\nsource_folder = \"/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test\"\ncategories = os.listdir(source_folder)\n# categories = categories.sort()\nprint(categories)\nprint(len(categories))      # Should be 131\n\n# Load and prepare image\ndef prepareImage(path):\n    img = load_img(path, target_size=(100,100))\n    imgArray = img_to_array(img)\n    # print(imgArray.shape)\n    imgArray = np.expand_dims(imgArray, axis=0)\n    imgArray = imgArray / 255.\n    return imgArray\n\ntestImgPath = \"/kaggle/input/eggplant/eggplant.jpg\"\nimageForModel = prepareImage(testImgPath)\n\nresultArray = model.predict(imageForModel, verbose=1)\nanswers = np.argmax(resultArray, axis=1)\nprint(categories[answers[0]])\n\n# Show image with text below\nimg = cv.imread(testImgPath)\ncv.putText(img, categories[answers[0]],(0,50), cv.FONT_HERSHEY_COMPLEX, 1, (209, 19, 77), 2)\ncv.imshow('img', img)\ncv.waitKey(0)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T21:09:07.305597Z","iopub.status.idle":"2023-07-13T21:09:07.305993Z","shell.execute_reply.started":"2023-07-13T21:09:07.305785Z","shell.execute_reply":"2023-07-13T21:09:07.305803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow-gpu==2.10\nfrom tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:25:29.944169Z","iopub.execute_input":"2023-07-14T20:25:29.944556Z","iopub.status.idle":"2023-07-14T21:05:35.648412Z","shell.execute_reply.started":"2023-07-14T20:25:29.944522Z","shell.execute_reply":"2023-07-14T21:05:35.645350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning 2","metadata":{}},{"cell_type":"code","source":"import keras_tuner as kt\nimport tensorflow as tf\nimport os\nfrom keras.utils import img_to_array, load_img\nimport numpy as np\nimport cv2 as cv\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\n\n#class MyHyperModel(kt.HyperModel):\ndef build(hp):\n    model = Sequential()\n    model.add(Conv2D(filters=128, kernel_size=3, activation=hp.Choice('activation', ['relu', 'PReLU', 'elu']), input_shape=(100,100,3)))\n    model.add(MaxPooling2D())\n    model.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\"))\n    model.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n    model.add(MaxPooling2D())\n    model.add(Dropout(rate=hp.Float('rate',min_value=0.3,max_value=0.8,step=0.1)))\n    model.add(Flatten())\n    model.add(Dense(units=hp.Int('units',3000,8000,500), activation=hp.Choice('activation_2', ['relu', 'PReLU', 'elu'])))\n    model.add(Dense(units=hp.Int('units_2',500,1500,250), activation=\"relu\"))\n    model.add(Dense(131, activation=\"softmax\"))\n    #model.add(keras.layers.Dense(\n     #   hp.Choice('units', [8, 16, 32]),\n      #  activation='relu'))\n   # model.add(keras.layers.Dense(1, activation='relu'))\n    model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n    \n    print(model.summary())\n    return model\n\n#def fit(hp, model):\n    # Early stopping\n#stop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\n#history = model.fit(train_generator, steps_per_epoch=stepsPerEpoch, epochs=5, validation_data=test_generator, validation_steps=validationSteps, callbacks=[stop_early])\n\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n#x = MyHyperMode(kt.HyperModel)\n#build(kt.HyperParameters())\n#tuner = kt.Hyperband(hypermodel = build, objective=\"val_accuracy\", max_epochs=10, hyperband_iterations=10,seed=1,max_retries_per_trial=0, max_consecutive_failed_trials=3)\ntuner = kt.RandomSearch(hypermodel = build, objective=\"val_accuracy\", max_trials=10,executions_per_trial=1,overwrite=False)\n#tuner.search_space_summary()\ntuner.search(train_generator,epochs=2,validation_data=test_generator)\ntuner.results_summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T17:57:55.346565Z","iopub.execute_input":"2023-07-16T17:57:55.346981Z","iopub.status.idle":"2023-07-16T19:25:22.615484Z","shell.execute_reply.started":"2023-07-16T17:57:55.346944Z","shell.execute_reply":"2023-07-16T19:25:22.613901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VGG 16","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom keras.models import Model\nfrom keras.utils import img_to_array, load_img\nfrom keras.optimizers import Adam\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Dense, Dropout, Flatten\nfrom pathlib import Path\n#from livelossplot.inputs.keras import PlotLossesCallback\nimport numpy as np\n\n#import cv2 as cv\n#from keras.models import Sequential\n#from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n#import matplotlib.pyplot as plt\n\n\nBATCH_SIZE = 64\n\n'''train_datagen = ImageDataGenerator(rotation_range=90, \n                                     brightness_range=[0.1, 0.7],\n                                     width_shift_range=0.5, \n                                     height_shift_range=0.5,\n                                     horizontal_flip=True, \n                                     vertical_flip=True,\n                                     validation_split=0.15,\n                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing\n'''\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n\ndef create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n    \"\"\"\n    Compiles a model integrated with VGG16 pretrained layers\n    \n    input_shape: tuple - the shape of input images (width, height, channels)\n    n_classes: int - number of classes for the output layer\n    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n    fine_tune: int - The number of pre-trained layers to unfreeze.\n                If set to 0, all pretrained layers will freeze during training\n    \"\"\"\n    \n    # Pretrained convolutional layers are loaded using the Imagenet weights.\n    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n    conv_base = VGG16(include_top=False,\n                     weights='imagenet', \n                     input_shape=input_shape)\n    \n    # Defines how many layers to freeze during training.\n    # Layers in the convolutional base are switched from trainable to non-trainable\n    # depending on the size of the fine-tuning parameter.\n    if fine_tune > 0:\n        for layer in conv_base.layers[:-fine_tune]:\n            layer.trainable = False\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n            \n    # Create a new 'top' of the model (i.e. fully-connected layers).\n    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n    top_model = conv_base.output\n    top_model = Flatten(name=\"flatten\")(top_model)\n    top_model = Dense(5000, activation='relu')(top_model)\n    top_model = Dense(1000, activation='relu')(top_model)\n    top_model = Dropout(0.2)(top_model)\n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    # Group the convolutional base and new fully-connected layers into a Model object.\n    model = Model(inputs=conv_base.input, outputs=output_layer)\n\n    # Compiles the model for training.\n    model.compile(optimizer=optimizer, \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    print(model.summary())\n    return model\n\ninput_shape = (100, 100, 3)\noptim_1 = Adam(learning_rate=0.001)\nn_classes=131\n\nn_steps = train_generator.samples // batchSize\nn_val_steps = test_generator.samples // batchSize\nn_epochs = 50\n\n# First we'll train the model without Fine-tuning\nvgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n\n\n# Early stopping\nstop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\nhistory = vgg_model.fit(train_generator, steps_per_epoch=n_steps, epochs=5, validation_data=test_generator, validation_steps=n_val_steps, callbacks=[stop_early])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:34:44.008337Z","iopub.execute_input":"2023-07-18T21:34:44.008700Z","iopub.status.idle":"2023-07-18T22:10:07.174176Z","shell.execute_reply.started":"2023-07-18T21:34:44.008671Z","shell.execute_reply":"2023-07-18T22:10:07.173137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inception","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom keras.models import Model\nfrom keras.utils import img_to_array, load_img\nfrom keras.optimizers import Adam\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Dense, Dropout, Flatten\nfrom pathlib import Path\n#from livelossplot.inputs.keras import PlotLossesCallback\nimport numpy as np\n\n#import cv2 as cv\n#from keras.models import Sequential\n#from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n#import matplotlib.pyplot as plt\n\n\nBATCH_SIZE = 64\n\n'''train_datagen = ImageDataGenerator(rotation_range=90, \n                                     brightness_range=[0.1, 0.7],\n                                     width_shift_range=0.5, \n                                     height_shift_range=0.5,\n                                     horizontal_flip=True, \n                                     vertical_flip=True,\n                                     validation_split=0.15,\n                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing\n'''\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n\ndef create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n    \"\"\"\n    Compiles a model integrated with VGG16 pretrained layers\n    \n    input_shape: tuple - the shape of input images (width, height, channels)\n    n_classes: int - number of classes for the output layer\n    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n    fine_tune: int - The number of pre-trained layers to unfreeze.\n                If set to 0, all pretrained layers will freeze during training\n    \"\"\"\n    \n    # Pretrained convolutional layers are loaded using the Imagenet weights.\n    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n    conv_base = InceptionV3(include_top=False,\n                     weights='imagenet', \n                     input_shape=input_shape)\n    \n    # Defines how many layers to freeze during training.\n    # Layers in the convolutional base are switched from trainable to non-trainable\n    # depending on the size of the fine-tuning parameter.\n    if fine_tune > 0:\n        for layer in conv_base.layers[:-fine_tune]:\n            layer.trainable = False\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n            \n    # Create a new 'top' of the model (i.e. fully-connected layers).\n    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n    top_model = conv_base.output\n    top_model = Flatten(name=\"flatten\")(top_model)\n    top_model = Dense(5000, activation='relu')(top_model)\n    top_model = Dense(1000, activation='relu')(top_model)\n    top_model = Dropout(0.2)(top_model)\n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    # Group the convolutional base and new fully-connected layers into a Model object.\n    model = Model(inputs=conv_base.input, outputs=output_layer)\n\n    # Compiles the model for training.\n    model.compile(optimizer=optimizer, \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    print(model.summary())\n    return model\n\ninput_shape = (100, 100, 3)\noptim_1 = Adam(learning_rate=0.001)\nn_classes=131\n\nn_steps = train_generator.samples // batchSize\nn_val_steps = test_generator.samples // batchSize\nn_epochs = 50\n\n# First we'll train the model without Fine-tuning\ninception_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n\n\n# Early stopping\nstop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\nhistory = inception_model.fit(train_generator, steps_per_epoch=n_steps, epochs=5, validation_data=test_generator, validation_steps=n_val_steps, callbacks=[stop_early])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-19T21:01:20.375626Z","iopub.execute_input":"2023-07-19T21:01:20.376030Z","iopub.status.idle":"2023-07-19T21:34:47.047515Z","shell.execute_reply.started":"2023-07-19T21:01:20.375998Z","shell.execute_reply":"2023-07-19T21:34:47.046451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#From MachineLearningMastery\n\n# example of creating a CNN with an efficient inception module\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.utils import plot_model\n \n# function for creating a projected inception module\ndef inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n # 1x1 conv\n conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n # 3x3 conv\n conv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu')(layer_in)\n conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(conv3)\n # 5x5 conv\n conv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(layer_in)\n conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(conv5)\n # 3x3 max pooling\n pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n pool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)\n # concatenate filters, assumes filters/channels last\n layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n return layer_out\n \n# define model input\nvisible = Input(shape=(256, 256, 3))\n# add inception block 1\nlayer = inception_module(visible, 64, 96, 128, 16, 32, 32)\n# add inception block 1\nlayer = inception_module(layer, 128, 128, 192, 32, 96, 64)\n# create model\nmodel = Model(inputs=visible, outputs=layer)\n# summarize model\nmodel.summary()\n# plot model architecture\nplot_model(model, show_shapes=True, to_file='inception_module.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DenseNet","metadata":{}},{"cell_type":"code","source":"# DenseNet 121\n\nimport tensorflow as tf\nimport os\nfrom keras.models import Model\nfrom keras.utils import img_to_array, load_img\nfrom keras.optimizers import Adam\nfrom keras.applications.densenet import DenseNet121, preprocess_input # Also have DenseNet169, DenseNet201\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Dense, Dropout, Flatten\nfrom pathlib import Path\n#from livelossplot.inputs.keras import PlotLossesCallback\nimport numpy as np\n\n#import cv2 as cv\n#from keras.models import Sequential\n#from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n#import matplotlib.pyplot as plt\n\n\nBATCH_SIZE = 64\n\n'''train_datagen = ImageDataGenerator(rotation_range=90, \n                                     brightness_range=[0.1, 0.7],\n                                     width_shift_range=0.5, \n                                     height_shift_range=0.5,\n                                     horizontal_flip=True, \n                                     vertical_flip=True,\n                                     validation_split=0.15,\n                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing\n'''\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n\ndef create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n    \"\"\"\n    Compiles a model integrated with VGG16 pretrained layers\n    \n    input_shape: tuple - the shape of input images (width, height, channels)\n    n_classes: int - number of classes for the output layer\n    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n    fine_tune: int - The number of pre-trained layers to unfreeze.\n                If set to 0, all pretrained layers will freeze during training\n    \"\"\"\n    \n    # Pretrained convolutional layers are loaded using the Imagenet weights.\n    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n    conv_base = DenseNet121(include_top=False,\n                     weights='imagenet', \n                     input_shape=input_shape)\n    \n    # Defines how many layers to freeze during training.\n    # Layers in the convolutional base are switched from trainable to non-trainable\n    # depending on the size of the fine-tuning parameter.\n    if fine_tune > 0:\n        for layer in conv_base.layers[:-fine_tune]:\n            layer.trainable = False\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n            \n    # Create a new 'top' of the model (i.e. fully-connected layers).\n    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n    top_model = conv_base.output\n    top_model = Flatten(name=\"flatten\")(top_model)\n    top_model = Dense(5000, activation='relu')(top_model)\n    top_model = Dense(1000, activation='relu')(top_model)\n    top_model = Dropout(0.2)(top_model)\n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    # Group the convolutional base and new fully-connected layers into a Model object.\n    model = Model(inputs=conv_base.input, outputs=output_layer)\n\n    # Compiles the model for training.\n    model.compile(optimizer=optimizer, \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    print(model.summary())\n    return model\n\ninput_shape = (100, 100, 3)\noptim_1 = Adam(learning_rate=0.001)\nn_classes=131\n\nn_steps = train_generator.samples // batchSize\nn_val_steps = test_generator.samples // batchSize\nn_epochs = 50\n\n# First we'll train the model without Fine-tuning\ndense_net_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n\n\n# Early stopping\nstop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\nhistory = dense_net_model.fit(train_generator, steps_per_epoch=n_steps, epochs=5, validation_data=test_generator, validation_steps=n_val_steps, callbacks=[stop_early])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Xception","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom keras.models import Model\nfrom keras.utils import img_to_array, load_img\nfrom keras.optimizers import Adam\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Dense, Dropout, Flatten\nfrom pathlib import Path\n#from livelossplot.inputs.keras import PlotLossesCallback\nimport numpy as np\n\n#import cv2 as cv\n#from keras.models import Sequential\n#from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n#import matplotlib.pyplot as plt\n\n\nBATCH_SIZE = 64\n\n'''train_datagen = ImageDataGenerator(rotation_range=90, \n                                     brightness_range=[0.1, 0.7],\n                                     width_shift_range=0.5, \n                                     height_shift_range=0.5,\n                                     horizontal_flip=True, \n                                     vertical_flip=True,\n                                     validation_split=0.15,\n                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing\n'''\n\n# Load data\ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.3, horizontal_flip=True, vertical_flip=True, zoom_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrainPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'\ntestPath = '/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test'\n\nbatchSize = 64      # Reduce value if you have less GPU\n\ntrain_generator = train_datagen.flow_from_directory(trainPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\", shuffle=True)\n\n\ntest_generator = test_datagen.flow_from_directory(testPath, target_size=(100,100), batch_size=batchSize, color_mode=\"rgb\", class_mode=\"categorical\")\n\ndef create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n    \"\"\"\n    Compiles a model integrated with VGG16 pretrained layers\n    \n    input_shape: tuple - the shape of input images (width, height, channels)\n    n_classes: int - number of classes for the output layer\n    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n    fine_tune: int - The number of pre-trained layers to unfreeze.\n                If set to 0, all pretrained layers will freeze during training\n    \"\"\"\n    \n    # Pretrained convolutional layers are loaded using the Imagenet weights.\n    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n    conv_base = Xception(include_top=False,\n                     weights='imagenet', \n                     input_shape=input_shape)\n    \n    # Defines how many layers to freeze during training.\n    # Layers in the convolutional base are switched from trainable to non-trainable\n    # depending on the size of the fine-tuning parameter.\n    if fine_tune > 0:\n        for layer in conv_base.layers[:-fine_tune]:\n            layer.trainable = False\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n            \n    # Create a new 'top' of the model (i.e. fully-connected layers).\n    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n    top_model = conv_base.output\n    top_model = Flatten(name=\"flatten\")(top_model)\n    top_model = Dense(5000, activation='relu')(top_model)\n    top_model = Dense(1000, activation='relu')(top_model)\n    top_model = Dropout(0.2)(top_model)\n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    # Group the convolutional base and new fully-connected layers into a Model object.\n    model = Model(inputs=conv_base.input, outputs=output_layer)\n\n    # Compiles the model for training.\n    model.compile(optimizer=optimizer, \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    print(model.summary())\n    return model\n\ninput_shape = (100, 100, 3)\noptim_1 = Adam(learning_rate=0.001)\nn_classes=131\n\nn_steps = train_generator.samples // batchSize\nn_val_steps = test_generator.samples // batchSize\nn_epochs = 50\n\n# First we'll train the model without Fine-tuning\nxception_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n\n\n# Early stopping\nstop_early = EarlyStopping(monitor=\"val_accuracy\", patience=5)      # Stop fitting model if it doesn't improve by 5\n\nhistory = xception_model.fit(train_generator, steps_per_epoch=n_steps, epochs=5, validation_data=test_generator, validation_steps=n_val_steps, callbacks=[stop_early])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# YOLO","metadata":{}},{"cell_type":"code","source":"#MachineLearningMastery\n\n# based on https://github.com/experiencor/keras-yolo3\nimport struct\nimport numpy as np\nfrom keras.layers import Conv2D\nfrom keras.layers import Input\nfrom keras.layers import BatchNormalization\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import UpSampling2D\nfrom keras.layers.merge import add, concatenate\nfrom keras.models import Model\n \ndef _conv_block(inp, convs, skip=True):\n x = inp\n count = 0\n for conv in convs:\n if count == (len(convs) - 2) and skip:\n skip_connection = x\n count += 1\n if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n x = Conv2D(conv['filter'],\n    conv['kernel'],\n    strides=conv['stride'],\n    padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n    name='conv_' + str(conv['layer_idx']),\n    use_bias=False if conv['bnorm'] else True)(x)\n if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n return add([skip_connection, x]) if skip else x\n \ndef make_yolov3_model():\n input_image = Input(shape=(None, None, 3))\n # Layer  0 => 4\n x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n   {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n   {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n   {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n # Layer  5 => 8\n x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n # Layer  9 => 11\n x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n # Layer 12 => 15\n x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n # Layer 16 => 36\n for i in range(7):\n x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n skip_36 = x\n # Layer 37 => 40\n x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n # Layer 41 => 61\n for i in range(7):\n x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n skip_61 = x\n # Layer 62 => 65\n x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n # Layer 66 => 74\n for i in range(3):\n x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n # Layer 75 => 79\n x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n # Layer 80 => 82\n yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n   {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n # Layer 83 => 86\n x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n x = UpSampling2D(2)(x)\n x = concatenate([x, skip_61])\n # Layer 87 => 91\n x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n # Layer 92 => 94\n yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n   {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n # Layer 95 => 98\n x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n x = UpSampling2D(2)(x)\n x = concatenate([x, skip_36])\n # Layer 99 => 106\n yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n    {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n    {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n    {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n    {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n    {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n    {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n return model\n \nclass WeightReader:\n def __init__(self, weight_file):\n with open(weight_file, 'rb') as w_f:\n major, = struct.unpack('i', w_f.read(4))\n minor, = struct.unpack('i', w_f.read(4))\n revision, = struct.unpack('i', w_f.read(4))\n if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n w_f.read(8)\n else:\n w_f.read(4)\n transpose = (major > 1000) or (minor > 1000)\n binary = w_f.read()\n self.offset = 0\n self.all_weights = np.frombuffer(binary, dtype='float32')\n \n def read_bytes(self, size):\n self.offset = self.offset + size\n return self.all_weights[self.offset-size:self.offset]\n \n def load_weights(self, model):\n for i in range(106):\n try:\n conv_layer = model.get_layer('conv_' + str(i))\n print(\"loading weights of convolution #\" + str(i))\n if i not in [81, 93, 105]:\n norm_layer = model.get_layer('bnorm_' + str(i))\n size = np.prod(norm_layer.get_weights()[0].shape)\n beta  = self.read_bytes(size) # bias\n gamma = self.read_bytes(size) # scale\n mean  = self.read_bytes(size) # mean\n var   = self.read_bytes(size) # variance\n weights = norm_layer.set_weights([gamma, beta, mean, var])\n if len(conv_layer.get_weights()) > 1:\n bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n kernel = kernel.transpose([2,3,1,0])\n conv_layer.set_weights([kernel, bias])\n else:\n kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n kernel = kernel.transpose([2,3,1,0])\n conv_layer.set_weights([kernel])\n except ValueError:\n print(\"no convolution #\" + str(i))\n \n def reset(self):\n self.offset = 0\n \n# define the model\nmodel = make_yolov3_model()\n# load the model weights\nweight_reader = WeightReader('yolov3.weights')\n# set the model weights into the model\nweight_reader.load_weights(model)\n# save the model to file\nmodel.save('model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# based on https://github.com/experiencor/keras-yolo3\nfrom numpy import expand_dims\nfrom keras.models import load_model\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n \n# load and prepare an image\ndef load_image_pixels(filename, shape):\n    # load the image to get its shape\n    image = load_img(filename)\n    width, height = image.size\n    # load the image with the required size\n    image = load_img(filename, target_size=shape)\n    # convert to numpy array\n    image = img_to_array(image)\n    # scale pixel values to [0, 1]\n    image = image.astype('float32')\n    image /= 255.0\n    # add a dimension so that we have one sample\n    image = expand_dims(image, 0)\n    return image, width, height\n \n# load yolov3 model\nmodel = load_model('model.h5')\n# define the expected input shape for the model\ninput_w, input_h = 416, 416\n# define our new photo\nphoto_filename = 'zebra.jpg'\n# load and prepare image\nimage, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n# make prediction\nyhat = model.predict(image)\n# summarize the shape of the list of arrays\nprint([a.shape for a in yhat])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}